{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News Sentiment Analysis using Google News\n",
    "https://colab.research.google.com/drive/1cUAxtQSkt51GWNFWleOcR-xEy1jub0zL#scrollTo=xWR4BHlJ-fC4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.getcwd()+\"/app\")\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from GoogleNews import GoogleNews\n",
    "from newspaper import Article\n",
    "from newspaper import Config\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "nltk.download('vader_lexicon') #required for Sentiment Analysis\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "now = dt.date.today()\n",
    "now = now.strftime('%m-%d-%Y')\n",
    "yesterday = dt.date.today() - dt.timedelta(days = 1)\n",
    "yesterday = yesterday.strftime('%m-%d-%Y')\n",
    "\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 12_3_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.3 Safari/605.1.15'\n",
    "config = Config()\n",
    "config.browser_user_agent = user_agent\n",
    "config.request_timeout = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the company name in a variable\n",
    "company_name = \"BPCL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#As long as the company name is valid not empty...\n",
    "if company_name != '':\n",
    "    print(f'Searching for and analyzing {company_name}, Please be patient, it might take a while...')\n",
    "\n",
    "    #Extract News with Google News\n",
    "    googlenews = GoogleNews(start=yesterday,end=now)\n",
    "    googlenews.search(company_name)\n",
    "    result = googlenews.result()\n",
    "    #store the results\n",
    "    df = pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    list =[] #creating an empty list \n",
    "    for i in df.index:\n",
    "        dict = {} #creating an empty dictionary to append an article in every single iteration\n",
    "        article = Article(df['link'][i],config=config) #providing the link\n",
    "        try:\n",
    "          article.download() #downloading the article \n",
    "          article.parse() #parsing the article\n",
    "          article.nlp() #performing natural language processing (nlp)\n",
    "        #   print(article.summary)\n",
    "        except:\n",
    "           pass \n",
    "        #storing results in our empty dictionary\n",
    "        dict['Date']=df['date'][i] \n",
    "        dict['Media']=df['media'][i]\n",
    "        dict['Title']=article.title\n",
    "        dict['Article']=article.text\n",
    "        dict['Summary']=article.summary\n",
    "        # print(\"-------- : \"+dict['Summary'])\n",
    "        dict['Key_words']=article.keywords\n",
    "        list.append(dict)\n",
    "    check_empty = not any(list)\n",
    "    print(check_empty)\n",
    "    if check_empty == False:\n",
    "      news_df=pd.DataFrame(list) #creating dataframe\n",
    "\n",
    "except Exception as e:\n",
    "    #exception handling\n",
    "    print(\"exception occurred:\" + str(e))\n",
    "    print('Looks like, there is some error in retrieving the data, Please try again or try with a different ticker.' )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df['Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    "    return 100 * float(part)/float(whole)\n",
    "\n",
    "#Assigning Initial Values\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "#Creating empty lists\n",
    "news_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "\n",
    "#Iterating over the tweets in the dataframe\n",
    "for news in news_df['Summary']:\n",
    "    news_list.append(news)\n",
    "    analyzer = SentimentIntensityAnalyzer().polarity_scores(news)\n",
    "    neg = analyzer['neg']\n",
    "    neu = analyzer['neu']\n",
    "    pos = analyzer['pos']\n",
    "    comp = analyzer['compound']\n",
    "\n",
    "    if neg > pos:\n",
    "        negative_list.append(news) #appending the news that satisfies this condition\n",
    "        negative += 1 #increasing the count by 1\n",
    "    elif pos > neg:\n",
    "        positive_list.append(news) #appending the news that satisfies this condition\n",
    "        positive += 1 #increasing the count by 1\n",
    "    elif pos == neg:\n",
    "        neutral_list.append(news) #appending the news that satisfies this condition\n",
    "        neutral += 1 #increasing the count by 1 \n",
    "\n",
    "positive = percentage(positive, len(news_df)) #percentage is the function defined above\n",
    "negative = percentage(negative, len(news_df))\n",
    "neutral = percentage(neutral, len(news_df))\n",
    "\n",
    "#Converting lists to pandas dataframe\n",
    "news_list = pd.DataFrame(news_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "#using len(length) function for counting\n",
    "print(\"Positive Sentiment:\", '%.2f' % len(positive_list), end='\\n')\n",
    "print(\"Neutral Sentiment:\", '%.2f' % len(neutral_list), end='\\n')\n",
    "print(\"Negative Sentiment:\", '%.2f' % len(negative_list), end='\\n')\n",
    "\n",
    "#Creating PieCart\n",
    "labels = ['Positive ['+str(round(positive))+'%]' , 'Neutral ['+str(round(neutral))+'%]','Negative ['+str(round(negative))+'%]']\n",
    "sizes = [positive, neutral, negative]\n",
    "colors = ['yellowgreen', 'blue','red']\n",
    "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
    "plt.style.use('default')\n",
    "plt.legend(labels)\n",
    "plt.title(\"Sentiment Analysis Result for stock= \"+company_name+\"\" )\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "# Word cloud visualization\n",
    "def word_cloud(text):\n",
    "    stopwords = set(STOPWORDS)\n",
    "    allWords = ' '.join([nws for nws in text])\n",
    "    wordCloud = WordCloud(background_color='black',width = 1600, height = 800,stopwords = stopwords,min_font_size = 20,max_font_size=150,colormap='prism').generate(allWords)\n",
    "    fig, ax = plt.subplots(figsize=(20,10), facecolor='k')\n",
    "    plt.imshow(wordCloud)\n",
    "    ax.axis(\"off\")\n",
    "    fig.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "print('Wordcloud for ' + company_name)\n",
    "word_cloud(news_df['Title'].values)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from news_sentiment.google_news_sentiment import GOOGLE_NEWS_SENTIMENT_ANALYSIS\n",
    "sentiment_analyser=GOOGLE_NEWS_SENTIMENT_ANALYSIS()\n",
    "\n",
    "# #  Get the news for the stock\n",
    "# news_url_df = sentiment_analyser.get_news_url_for_stock(company_name)\n",
    "# print(news_url_df.head())\n",
    "# # Extract the keywords from the news\n",
    "# tokenised_article_df = sentiment_analyser.extract_news_keywords(company_name)\n",
    "# tokenised_article_df.head()\n",
    "\n",
    "sentiment_analyser.sentiment_analysis_plot(stock_symbol=company_name)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "528325db8fa72b779d77efa1487d29f7bc4a5df903bdf9226f78268fb75b900f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('streamlit-app-venv': virtualenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
